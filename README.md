# Work

ë°ì´í„°ë¥¼ ì˜¬ë¦´ ìˆ˜ëŠ” ì—†ì§€ë§Œ.. ì½”ë“œëŠ” ì •ë¦¬í•´ì„œ ì˜¬ë¦¬ëŠ” ê³³

[ğŸ](https://github.com/mizykk/Work/blob/master/Convert_Hex_color_to_Color_name.ipynb) Convert_Hex_color_to_Color_name : Hex colorë¥¼ ìƒ‰ìƒëª…ìœ¼ë¡œ ë³€ê²½  
[ğŸ](https://github.com/mizykk/Work/blob/master/Find_duplicates.ipynb) Find_duplicates : ì¤‘ë³µì œê±° & ì¤‘ë³µì°¾ê¸°   
[ğŸŠ](https://github.com/mizykk/Work/blob/master/Find_values_in_nested_dictionary.ipynb) Find_values_in_nested_dictionary : ì¤‘ì²© ë”•ì…”ë„ˆë¦¬ì—ì„œ ê°’ ì°¾ê¸°   
[ğŸ‹](https://github.com/mizykk/Work/blob/master/Find_words_in_sentences.ipynb) Find_words_in_sentences : ë¬¸ì¥ ì†ì—ì„œ ë‹¨ì–´ ì°¾ê¸°   
[ğŸ‰](https://github.com/mizykk/Work/blob/master/Getting_RGB_values_from_image.ipynb) Getting_RGB_values_from_image : ì´ë¯¸ì§€ì—ì„œ RGB ì¶”ì¶œí•˜ê¸°(í™”ì¥í’ˆ ìƒ‰ìƒ)  
[ğŸ‡](https://github.com/mizykk/Work/blob/master/Hit_path.ipynb) Hit_Path : GA hitsPath ì •ì œ     
[ğŸ“](https://github.com/mizykk/Work/blob/master/Movie_check.ipynb) Movie_check : ì˜í™” ê°œë´‰ì¼ì´ 2ê°œì›” ì´í›„ì¸ì§€ & ì„±ì¸ì˜í™”ì¸ì§€ íŒë‹¨  
[ğŸ’](https://github.com/mizykk/Work/blob/master/URL_Encoding.ipynb) URL_Encoding : URL ì¸ì½”ë”©     
[ğŸ‘](https://github.com/mizykk/Work/blob/master/Word_Cloud.ipynb) Word_Cloud : ì›Œë“œí´ë¼ìš°ë“œ   

---   
   
### ğŸ° python ğŸ°  
`len()` : ê¸¸ì´     
`lower()` : ì†Œë¬¸ìë¡œ  
`upper()` : ëŒ€ë¬¸ìë¡œ
`strip()` : ë¬¸ìì—´ ì–‘ìª½ ê³µë°± ì œê±°í•˜ê¸°    
`split()` : ë¬¸ìì—´ ë‚˜ëˆ„ê¸°    
`replace()` : ë¬¸ìì—´ ë³€ê²½í•˜ê¸°     
`append()` : ë¦¬ìŠ¤íŠ¸ì— ê°’ ì¶”ê°€í•˜ê¸°   
`lambda`    
`map`  
`apply`  
   
### ğŸ¼ Pandas ğŸ¼   
`import pandas as pd`  
`pd.read_csv()` : csvíŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°  
`pd.read_excel()` : ì—‘ì…€(xlsx)íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°  
`df.dtypes` : ë°ì´í„° íƒ€ì… í™•ì¸í•˜ê¸°  
`.head()` : ìƒìœ„ nê°œ ê°’ë§Œ ë³´ì´ê¸°    
`.tail()` : í•˜ìœ„ nê°œ ê°’ë§Œ ë³´ì´ê¸°    
`.sample()` : ëœë¤ìœ¼ë¡œ nê°œ ê°’ë§Œ ë³´ì´ê¸° 
`.unique()` : ê³ ìœ ê°’  
`pd.isnull()` : ê²°ì¸¡ê°’ì´ ìˆëŠ” ê²ƒ  
`pd.notnull()` : ê²°ì¸¡ê°’ ì•„ë‹Œ ê²ƒë§Œ ë³´ì—¬ì£¼ê¸°  
`df.fillna()` : ê²°ì¸¡ê°’ ì±„ìš°ê¸°    
`pd.reset_index()` : ì¸ë±ìŠ¤ ì´ˆê¸°í™”  
`pd.merge(df1, df2, by = , how = )` : ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°  
`pd.concat([df1, df2], axis = )` : ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°  
`pd.DataFrame()` : ë°ì´í„°í”„ë ˆì„ ë§Œë“¤ê¸°  
`pd.pivot_table()` : í”¼ë´‡í…Œì´ë¸”   
`df.apply()` : í•¨ìˆ˜ í•œë²ˆì— ì ìš©í•˜ê¸°    
`pd.drop_duplicates(['col'], keep = )` : ì¤‘ë³µì œê±°í•˜ê¸°    
`.isin()` : Aê°€ Bì•ˆì— ë“¤ì–´ìˆëŠ”ì§€    
`df.to_csv()` : csvë¡œ ë‚´ë³´ë‚´ê¸°  
`df.to_excel()` : ì—‘ì…€(xlsx)íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°  

### ğŸ¦ re ğŸ¦     
`import re`   
`re.compile('exp')`  
`re.sub('after', 'before')` : ë¬¸ìì—´ ë³€ê²½í•˜ê¸° 
`re.findall()` : ì»´íŒŒì¼ ëœ ì •ê·œì‹ê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ìë¥¼ ì°¾ì•„ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜     

### ğŸ¦‹ urllib ğŸ¦‹  
`from urllib import parse`  
`parse.urlparse()` : Url parsing  
`parse.parse_qs()` : queryë¥¼ íŒŒì‹±í•´ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜  
`parse.parse_qsl()` : queryë¥¼ íŒŒì‹±í•´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜  
`parse.urlencode()` : Encoding  
  
### ğŸ¹ Datetime ğŸ¹  
`from datetime import datetime`
`datetime.datetime.now()` : í˜„ì¬ì‹œê°  
`datetime.date()` : ë‚ ì§œë§Œ ì¶œë ¥  
`datetime.time()` : ì‹œê°„ë§Œ ì¶œë ¥   
`datetime.strftime()` : datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì¤€ë‹¤.     
`datetime.strptime(date_string, format)` : ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜í•´ì¤€ë‹¤.    
  
### ğŸ¦Š Crawling ğŸ¦Š  
`import requests
from bs4 import BeautifulSoup`
`rq = requests.get(url)` : Get request    
`html = rq.text` : HTML ê°€ì ¸ì˜¤ê¸°  
`bs = BeautifulSoup(html, 'html.parser')` : HTML Parsing     
`bs.find(tag).text` : tagì— í•´ë‹¹í•˜ëŠ” ë¬¸ì ê°€ì ¸ì˜¤ê¸°  
`bs.find('div', class_ = 'í´ë˜ìŠ¤ëª…')` : Classë¡œ ì°¾ê¸°  
`bs.find('div', id = 'idëª…')` : Idë¡œ ì°¾ê¸°  
    
### ğŸ® Connect with Database ğŸ®  
ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°    
`import pymysql  
from sqlalchemy import create_engine  
   
engine = create_engine(f'mysql+pymysql://{user_nm}:{passwd}@{host_url}:{port_num}/{db_name}?charset=utf8')  
engine_conn = engine.connect()  
   
data = pd.read_sql("""  
    Query  
    """, engine_conn)  
   
engine_conn.close()`

ë°ì´í„°ë² ì´ìŠ¤ì— í…Œì´ë¸” ì—…ë¡œë“œ  
`engine = create_engine(f'mysql+pymysql://{user_nm}:{passwd}@{host_url}:{port_num}/{db_name}?charset=utf8')  
engine_conn = engine.connect()  
data.to_sql(table_name, engine_conn, if_exists='replace', index=None)  
engine_conn.close()  
engine.dispose()`
if_exist = {'replace', 'append', 'fail')   


### ğŸ¹ And more.. ğŸ¹   
ê²½ê³  ì•ˆ ë‚˜íƒ€ë‚˜ê²Œ     
`import warnings`       
`warnings.filterwarnings(action='ignore')`  
     
    
ë©€í‹°í”„ë¡œì„¸ì‹±  
`if __name__=='__main__':  
    pool = Pool(processes=12)  
    result = pool.map(get_color_name, range(0, len(data)))`  


ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ    
`from ast import literal_eval`   
`literal_eval()`


URLë¡œ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°   
`from PIL import Image
from io import BytesIO
response = requests.get(data['palette'][i])  
img = Image.open(BytesIO(response.content))  
col = img.load()`    

í† í°í™” & Stopwords ì œê±°  
`from tensorflow.keras.preprocessing.text import text_to_word_sequence
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
nltk.download('wordnet')

def remove_stopwords(words):
    words = [w for w in words if w not in stopwords.words('english')]
    return words
    
text_to_word_sequence(x))`  
  
ë³µìˆ˜í˜•ì„ ë‹¨ìˆ˜ë¡œ ë§Œë“¤ê¸°   
`from nltk.stem import WordNetLemmatizer 
lemm = WordNetLemmatizer()
emm.lemmatize()`
